---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Data Processing and Visualization

```{r echo=FALSE}
library(reticulate)
use_python("C:/Users/m_tai/anaconda3/python.exe", required = TRUE)
```


## Library Installation

Installing libraries in Python is essential to expand the language's functionality. There are several ways to install libraries, but the most common is using a package manager. Pip is the default package manager for Python and usually comes with the Python installation.

To install a library with pip, open the terminal or command prompt and type the following command:

```{python}
#| eval: false
pip install library_name
```

Replace `library_name` with the name of the library you want to install.

## Numerical Data Processing

`NumPy` (Numerical Python) is an essential library for numerical computing in Python. It provides efficient data structures for working with multidimensional arrays and powerful mathematical functions for data manipulation.

To install NumPy, you can use pip, the default Python package manager:

```{python}
#| eval: false
pip install numpy
```

The main object in `NumPy` is the multidimensional array. You can create `NumPy` arrays using the `numpy.array()` function and perform basic mathematical operations with them:

```{python}
import numpy as np

# Creating a NumPy array
arr = np.array([1, 2, 3, 4, 5])

# Basic mathematical operations
print("Sum:", np.sum(arr))
print("Mean:", np.mean(arr))
```

In addition to basic operations, NumPy provides universal functions (`ufuncs`) to apply operations to all elements of an array at once:

```{python}
# Universal functions (ufuncs)
arr = np.array([1, 2, 3, 4, 5])
print("Square of each element:", np.square(arr))
print("Exponential of each element:", np.exp(arr))

```

Next, we present three practical examples of using `NumPy` functions.

**Example 1: Descriptive Statistics Calculation**

`NumPy` can be used to calculate descriptive statistics, such as mean, median, standard deviation, minimum, and maximum of time series data, such as the stock price of a company over time.

```{python}
import numpy as np

# Stock price of a company over time (in dollars)
prices = np.array([100, 102, 105, 110, 108, 115, 120])

# Calculating descriptive statistics
print("Mean:", np.mean(prices))
print("Standard deviation:", np.std(prices))
print("Maximum:", np.max(prices))
print("Minimum:", np.min(prices))
```

**Example 2: Time Series Analysis** `NumPy` is useful for time series manipulation and analysis. For example, you can use `NumPy` to calculate the return rate of an investment over time or to smooth a time series using moving averages.

```{python}
prices = np.array([100, 102, 105, 110, 108, 115, 120])
# Calculating the return rate of an investment over time
returns = np.diff(prices) / prices[:-1] * 100
print("Return rate:", returns)
```

::: callout-tip
The `np.diff` function in NumPy is used to calculate the difference between consecutive elements along a specified axis of an array. *Note*: The size of the output from the `np.diff` function will always be one less than the size of the original input vector. For example, if we have a one-dimensional vector with $n$ elements, the `np.diff` function will return a vector with $nâˆ’1$ elements, as there is no difference for the last element.
:::

```{python}
prices = np.array([100, 102, 105, 110, 108, 115, 120])
# Smoothing a time series using moving averages
window_size = 3
moving_average = np.convolve(prices, np.ones(window_size) / window_size, mode='valid')
print("Moving averages:", moving_average)
```

::: callout-tip
The `np.convolve` function in `NumPy` is used to perform convolution between two sequences, represented by two one-dimensional vectors. Convolution is a mathematical operation that combines two functions to produce a third function that represents the amount of overlap between them as one is shifted along the axis.

The basic syntax of the function is `np.convolve(a, b, mode='full')`, where `a` and `b` are the two one-dimensional vectors to be convolved, and `mode` is an optional parameter that defines the convolution mode. The most common modes are:

-   `'full'`: Returns the full output of the convolution. The length of the result will be `len(a) + len(b) - 1`.
-   `'valid'`: Returns only points where the sequences fully overlap. The length of the result will be `max(len(a), len(b)) - min(len(a), len(b)) + 1`.
-   `'same'`: Returns output of the same size as the longest input vector. The length of the result will be `max(len(a), len(b))`.

In the previous example, the `np.convolve` function was used here to calculate the moving average of prices. In this case, the first sequence is the price vector, and the second sequence is a vector of 1s divided by the moving average window size. This creates a sequence that represents a weighted average of the values.
:::

**Example 3: Monte Carlo Simulation** NumPy can be used to perform Monte Carlo simulations, which are widely used in financial modeling and risk assessment. For example, you can simulate the performance of an investment portfolio over time under different market scenarios.

```{python}
# Monte Carlo simulation of an investment portfolio's performance
num_simulations = 1000
num_years = 10
mean_return = 0.08
volatility = 0.15

# Generating random returns using a normal distribution
returns = np.random.normal(mean_return, volatility, size=(num_simulations, num_years))

# Calculating the final portfolio value for each simulation
initial_investment = 10000
final_values = initial_investment * np.cumprod(1 + returns, axis=1)

# Descriptive statistics of the final portfolio values
print("Mean final value:", np.mean(final_values[:,-1]))
print("Standard deviation of final values:", np.std(final_values[:,-1]))
```

More references on `NumPy`:

-   **Official NumPy Documentation:** <https://numpy.org/doc/stable/> The official NumPy documentation contains detailed information on all available functions and methods, as well as tutorials and examples.

-   **NumPy Quickstart Tutorial:** <https://numpy.org/doc/stable/user/quickstart.html> This quickstart tutorial provides a quick introduction to NumPy and its basic functionalities.

## Data Analysis and Processing

Pandas is a widely used open-source library in Python for data analysis and manipulation. It provides flexible data structures and powerful tools for working with structured data, facilitating data analysis, cleaning, and preparation for various applications, such as data science, finance, academic research, and more.

## What is Pandas?

Pandas is an open-source Python library that offers high-performance data structures and data analysis tools. Pandas is designed to handle the complexities of real-world data analysis, offering a simple and intuitive interface for working with tabular data.

Pandas is widely used in data analysis due to its ability to:

-   Import and export data from a variety of sources, including CSV, Excel, SQL, JSON, HDF5 files, and more.
-   Efficiently manipulate data, including indexing, filtering, aggregation, and cleaning.
-   Perform statistical and mathematical operations on data, such as mean, sum, standard deviation, correlation, etc.

The two main data structures provided by Pandas are Series and DataFrames.

### Series

A `Series` is a one-dimensional data structure that can contain any type of data, such as integers, floats, strings, among others. Each element in a Series has a unique label called an index. The Series is similar to a list or one-dimensional array in Python but provides additional features, such as vectorized operations and automatic data alignment based on index labels.

Suppose we have a Series representing the daily prices of a stock:

| Date       | Price |
|------------|-------|
| 2024-03-18 | 100   |
| 2024-03-19 | 105   |
| 2024-03-20 | 98    |
| 2024-03-21 | 102   |

We can create a Pandas Series to represent this data:

```{python}
import pandas as pd

# Stock price data
data = ['2024-03-18', '2024-03-19', '2024-03-20', '2024-03-21']
prices = [100, 105, 98, 102]

# Creating a Pandas Series
stock_price_series = pd.Series(prices, index=pd.to_datetime(data), name='Stock Price')
print(stock_price_series)
```

### DataFrame

A DataFrame is a two-dimensional data structure similar to a database table or an Excel spreadsheet. It consists of rows and columns, where each column can contain a different data type. Each column and row in a DataFrame has a unique label called an index and a name, respectively. The DataFrame allows for a wide range of data manipulation and analysis operations, such as indexing, filtering, aggregation, cleaning, among others.

Suppose we have a DataFrame representing the daily prices of multiple stocks. We can create a Pandas DataFrame to represent this data. See the example below.

```{python}
import pandas as pd
import numpy as np

# Stock price data
data = ['2024-03-18', '2024-03-19', '2024-03-20', '2024-03-21']
stock_prices = {
    'Stock 1': [100, 105, np.nan, 102],
    'Stock 2': [50, 52, 48, 49],
    'Stock 3': [75, np.nan, 72, 74]
}

# Creating a Pandas DataFrame
df_stock_prices = pd.DataFrame(stock_prices, index=pd.to_datetime(data))
print(df_stock_prices)

```

### Main Functionalities

The `df.isna()` function is a function provided by Pandas in a DataFrame (df) that returns a boolean matrix indicating whether each element in the DataFrame is a missing value (NaN).

When applied to a DataFrame, the `isna()` function returns a DataFrame with the same shape, where each value is replaced by `True` if it is `NaN` and `False` otherwise.

This is useful for quickly identifying missing values in a DataFrame and performing data cleaning or handling operations, such as filling missing values or removing rows or columns containing these values.

If we apply `df_stock_prices.isna()`, we get:

```{python}
df_stock_prices.isna()
```

To count the number of `NaN` values in each column, combine `is.na()` with `sum()`:

```{python}
df_stock_prices.isna().sum()
```

The `dropna()` method in Pandas is used to remove rows or columns that contain missing values (NaN).

```{python}
df_stock_prices.dropna()
```

The `subset` parameter is used to specify in which columns or rows Pandas should look for missing values before removing. When we use `df.dropna(subset=["Stock 3"])`, we are instructing Pandas to remove all rows where there is a missing value in the "Stock 3" column.

```{python}
df_stock_prices.dropna(subset=["Stock 3"])
```

In the `dropna()` function, the `inplace=True` parameter specifies that the modification should be made directly to the original DataFrame, rather than returning a new DataFrame without the missing values. When `inplace=True` is used with `dropna()`, the original DataFrame is modified, and the rows or columns with missing values are permanently removed.

```{python}
df_stock_prices.dropna(inplace = True)
```

The `fillna()` function in Pandas is used to fill missing values (NaN) in a DataFrame with a specific value.

Consider the following DataFrame `df` representing customer data from a bank with some missing data:

```{python}
import pandas as pd
import numpy as np

data = {'Name': ["John", "Mary", "Peter", "Anna", "Marianne"],
        'Age': [25, 30, np.nan, 40, 35],
        'Monthly Income': [5000, 6000, np.nan, 4500, 5500],
        'Credit Limit': [10000, np.nan, 8000, 12000, np.nan]}
df_customers = pd.DataFrame(data)
```

In this example,

-   The missing values in the "Age" column will be filled with the median of the existing ages in the DataFrame.
-   The missing values in the "Credit Limit" column will be filled with the mode of the existing credit limits in the DataFrame.
-   The missing values in the "Monthly Income" column will be filled with the mean of the existing monthly incomes in the DataFrame.

See the code below.

```{python}
# Filling missing values in the 'Age' column with the median of existing ages
df_customers['Age'] = df_customers['Age'].fillna(df_customers['Age'].median())

# Filling missing values in the 'Credit Limit' column with the mode of existing credit limits
df_customers['Credit Limit'] = df_customers['Credit Limit'].fillna(df_customers['Credit Limit'].mode()[0])

# Filling missing values in the 'Monthly Income' column with the mean of existing monthly incomes
df_customers['Monthly Income'] = df_customers['Monthly Income'].fillna(df_customers['Monthly Income'].mean())

df_customers
```

Now, let's load the `gapminder` data, which is in the `gapminder.zip` file.

```{python}
gapminder = pd.read_csv("data/gapminder.zip", sep = "\t")
```

The `head()` function is used to view the first few rows of the `gapminder` dataset, providing a quick overview of its structure and content.

```{python}
gapminder.head()
```

The `info()` method provides information about the dataset, including the number of entries, the data type of each column, and whether there are any null values.

```{python}
gapminder.info()
```

The `describe()` function generates descriptive statistics for each numeric column in the dataset, such as count, mean, standard deviation, minimum, and maximum.

```{python}
gapminder.describe()
```

The `value_counts()` function counts the number of occurrences of each category in the "continent" column of the `gapminder` dataset, transforms the results into a DataFrame, renames the columns to "continent" and "n" (indicating the count), and resets the index.

```{python}
gapminder.value_counts("continent").to_frame("n").reset_index()
```

In the following snippet, we perform a value count for the unique combinations of categories in the "continent" and "year" columns of the `gapminder` DataFrame. The results are transformed into a DataFrame, renamed as "continent", "year", and "n" (indicating the count), and the index is reset.

```{python}
gapminder.value_counts(["continent", "year"]).to_frame("n").reset_index()
```

## Tidy Data

All the tables below contain the same data (taken from the `tidyr` package in R), which shows the number of cases of a disease and the total population of some countries.

```{python}
table1 = pd.read_csv("data/table1.csv")
table2 = pd.read_csv("data/table2.csv")
table3 = pd.read_csv("data/table3.csv")
table4a = pd.read_csv("data/table4a.csv")
table4b = pd.read_csv("data/table4b.csv")
```

```{python}
table1
```

```{python}
table2
```

```{python}
table3
```

```{python}
table4a
```

```{python}
table4b
```

The example below creates a new column called `rate` in the `table1` DataFrame. The `assign` function adds a new column to the DataFrame, while the `lambda` expression calculates the values for this new column.

```{python}
table1.assign(rate = lambda _: 10000 * (_.cases / _.population))
```

In the example below, we group the data in the `table1` DataFrame by the "year" column and then calculate the sum of cases for each year. The `groupby("year")` method groups the data by year, creating separate groups for each year. `as_index = False` specifies that the column used for grouping ("year") should not be set as the index in the resulting DataFrame. The `agg` method is used to perform an aggregation operation on the groups. Here, `np.sum` is used to calculate the sum of the values in the "cases" column for each group.

```{python}
#| warning: false
(table1.groupby("year", as_index = False)
       .agg(total_cases = ("cases", np.sum)))
```

To do the same with the data from `table1`, we have to use the `pivot_table` function:

```{python}
#| warning: false
table2_tidy = (table2.pivot_table(index = ["country", "year"], columns = "type", values = "count")
               .reset_index()
               .rename_axis(None, axis = 1))

table2_tidy.assign(rate = lambda _: 10000 * (_.cases / _.population))
```

In the example above, we use the `pivot_table` method from Pandas to reorganize the data in the `table2` DataFrame. It reorganizes the data so that the values in the "count" column are pivoted (transformed into columns) based on the unique values of the combination of "country" and "year". The `index`, `columns`, and `values` parameters specify the columns to be used as the index, the columns to be transformed into columns, and the values to be filled in the pivot table, respectively. After the pivot operation, additional methods are chained to modify the structure of the resulting DataFrame:

-   `reset_index()` resets the DataFrame's indices to default numeric indices, moving the previous indices (in this case, "country" and "year") to columns.
-   `rename_axis(None, axis=1)` removes the column index names, replacing them with `None`. This is done specifically to clean up the column names of the DataFrame.

After transforming the data, the `assign` function is used to create a new column called `rate` in the resulting DataFrame `table2_tidy`.

Now, let's do the same for `table4a` and `table4b`:

```{python}
#| warning: false
table4_tidy = (table4a.melt(id_vars = "country", value_vars = ["1999", "2000"], var_name = "year", value_name = "cases")
                .merge(table4b.melt(id_vars = "country", value_vars = ["1999", "2000"], var_name = "year", value_name = "population"),
                       on = ("country", "year")))

table4_tidy.assign(rate = lambda _: 10000 * (_.cases / _.population))
```

The DataFrames `table4a` and `table4b` are melted using the `melt` method:

- For `table4a`, the columns that will remain fixed are specified through the argument `id_vars = "country"`, while the columns "1999" and "2000" are melted as variables using `value_vars = ["1999", "2000"]`. The names of the melted variables are renamed to "year" and "cases" using `var_name = "year"` and `value_name = "cases"`, respectively.

- Similarly, for `table4b`, the columns "country" and "1999", "2000" are melted, with the variable names renamed to "year" and "population", respectively.

The DataFrames resulting from the melting of `table4a` and `table4b` are merged using the `merge` method. The merge is performed based on the columns "country" and "year", ensuring that corresponding data from `table4a` and `table4b` are correctly combined.

Finally, the `assign` method is used to create a new column called "rate", which represents the number of cases per 10,000 inhabitants.

For `table3`, just split the `cases` column using the separator `/`:

```{python}
#| warning: false
print(table3)

table3_tidy = (table3.assign(cases = lambda _: _.rate.str.split("/", expand = True)[0].astype(int),
                       population = lambda _: _.rate.str.split("/",  expand = True)[1].astype(int))
               .drop("rate", axis = 1))

table3_tidy

table3_tidy.assign(rate = lambda _: 10000 * (_.cases / _.population))
```

The `expand` parameter is used in the `str.split()` method to specify whether the split result should be expanded into a DataFrame (if `True`) or kept as a list of values (if `False`, which is the default).

Main functionalities - see Paulo's lectures

<!-- lectures 4, 5 (data wrangling) and 6 (missing data) -->

## Data Visualization

Two widely used libraries for visualization in Python are `Matplotlib` and `Plotnine`. `Matplotlib` offers a wide range of options to create static visualizations, from simple charts to complex and customized graphs. On the other hand, `Plotnine` is a library based on the grammar of graphics (similar to R's `ggplot2`), which makes it easy to create elegant and concise visualizations using an intuitive and expressive syntax.

### Matplotlib

Before we start creating visualizations, it's important to understand some basic concepts of Matplotlib:

- **Figure and Axes**: In Matplotlib, a figure is the window or page where everything is drawn. Within a figure, there can be multiple axes (or subplots), where data is effectively plotted.

- **The plot() Method**: The `plot()` method is used to create line, point, or marker charts. It accepts various arguments to customize the appearance of the graph, such as color, line style, line width, etc.

- **Customization**: Matplotlib offers many customization options to adjust the appearance of charts, including adding axis labels, chart titles, legends, and more.

Now let's see an example of how to create a scatter plot using fictitious data, where each data unit is related to a company.

```{python}
#| warning: false
import matplotlib.pyplot as plt

# Example data: Company names, revenue, and number of employees
companies = ['A', 'B', 'C', 'D', 'E']
revenue = [200, 300, 400, 250, 350]  # in millions of reais
employees = [1000, 1200, 1500, 900, 1100]

# Creating the scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(revenue, employees, color='blue', marker='o')

# Adding labels and title
plt.xlabel('Revenue (in millions of reais)')
plt.ylabel('Number of Employees')
plt.title('Revenue vs. Number of Employees')

# Adding annotations for each point
for i in range(len(companies)):
    plt.annotate(companies[i], (revenue[i], employees[i]))

# Displaying the plot
plt.grid(True)
plt.show()
```

In this example, each point on the graph represents a company, where the x-axis represents revenue (in millions of reais) and the y-axis represents the number of employees. Annotations are used to identify each company on the graph.

Next, we use Matplotlib to create a line graph representing the sales evolution of two products over several years. Each point on the graph represents the number of sales in a specific year.

```{python}
#| warning: false
import matplotlib.pyplot as plt

# Example data: Years and product sales
years = [2010, 2011, 2012, 2013, 2014, 2015, 2016]
product_A_sales = [500, 600, 550, 700, 800, 750, 900]
product_B_sales = [400, 450, 500, 550, 600, 650, 700]

# Creating the line plot
plt.figure(figsize=(10, 6))
plt.plot(years, product_A_sales, marker='o', label='Product A')
plt.plot(years, product_B_sales, marker='s', label='Product B')

# Adding labels and title
plt.xlabel('Year')
plt.ylabel('Sales')
plt.title('Sales over Time')

# Adding legend
plt.legend()

# Displaying the plot
plt.grid(True)
plt.show()
```

### Plotnine

Plotnine is a Python library that allows the creation of statistical data visualizations in a simple and concise way, using the grammar of graphics from R (also known as `ggplot2`). This grammar consists of a declarative approach to chart construction, where visual elements are added in layers to form the final graph.

```{python}
#| warning: false
#| eval: false
from plotnine import *
```

To illustrate, let's use the `gapminder` dataset.

```{python}
#| warning: false
#| eval: false
((ggplot(gapminder, aes(x = "continent", fill = "continent")) +
     geom_bar(aes(y = "stat(count) / 12"), alpha = 0.75) +
     labs(x = "", y = "Number of countries", title = "Continents") +
     theme(legend_position = "none") +
     coord_flip()+
     theme_bw())
     .show())
```

```{python}
#| warning: false
#| eval: false
((ggplot(gapminder, aes(x = "lifeExp", y = "stat(density)")) +
     geom_histogram(fill = "blue", color = "white", alpha = 0.5) +
     labs(x = "Life Expectancy", y = "", title = "Gapminder"))
     .show())
```

```{python}
#| warning: false
#| eval: false
((ggplot(gapminder, aes(x = "lifeExp", y = "stat(density)")) +
     geom_histogram(fill = "blue", color = "white", alpha = 0.5) +
     labs(x = "Life Expectancy", y = "", title = "Gapminder") +
     facet_wrap("~ continent", nrow = 1) +
     theme(figure_size = (12, 2))).
     show)
```

```{python}
#| warning: false
#| eval: false
((gapminder.groupby(["continent", "year"], as_index = False)
      .agg(median_lifeExp = ("lifeExp", np.median))
      .pipe(lambda _: ggplot(_, aes(x = "year", y = "median_lifeExp", color = "continent")) +
                          geom_line(size = 0.75) +
                          geom_point(size = 1.5) +
                          labs(x = "Year", y = "Median Life Expectancy", color = "Continent", title = "Gapminder")))
                          .show())
```

<!-- ## Advanced Data Analysis -->

<!-- ### Case 1: Visualizing `NumPy` Data with `Matplotlib` {.unnumbered} -->

<!-- The objective of this example is to demonstrate how to use NumPy and Matplotlib together to analyze simulated economic data. Read the code below and try to understand what is being done. -->

```{python}
#| warning: false
#| include: false
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Generating random data to simulate GDP values
np.random.seed(42)
gdp = np.random.normal(500, 100, 100)  # Simulating 100 GDP values

# Calculating mean, median, and standard deviation of GDP
mean_gdp = np.mean(gdp)
median_gdp = np.median(gdp)
std_dev_gdp = np.std(gdp)

print("Mean GDP:", mean_gdp)
print("Median GDP:", median_gdp)
print("Standard Deviation of GDP:", std_dev_gdp)

# Plotting a histogram of GDP values
plt.hist(gdp, bins=10, color='skyblue', edgecolor='black')
plt.title('GDP Distribution')
plt.xlabel('GDP')
plt.ylabel('Frequency')
plt.show()
```

<!-- In the example above, we start by generating random data to simulate GDP values using the normal distribution with a mean of 500 and a standard deviation of 100. Next, we calculate the mean, median, and standard deviation of these values using NumPy functions. To facilitate visualization and data analysis, we plot a histogram using Matplotlib. This histogram allows us to visualize the distribution of GDP values and identify trends or patterns in the data. -->

<!-- ### Case 2: Gapminder {.unnumbered} -->

<!-- The objective of this example is to demonstrate how to use Pandas, NumPy, and Matplotlib together to perform exploratory data analysis on the Gapminder dataset. -->

```{python}
#| warning: false
#| eval: false
#| include: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Calculate the mean life expectancy using NumPy
mean_life_expectancy = np.mean(gapminder['lifeExp'])

# Calculate the median GDP per capita using NumPy
median_gdp_per_capita = np.median(gapminder['gdpPercap'])

# Plot a histogram of life expectancy using Matplotlib
plt.hist(gapminder['lifeExp'], bins=20, color='skyblue', edgecolor='black')
plt.axvline(mean_life_expectancy, color='red', linestyle='dashed', linewidth=1)
plt.text(mean_life_expectancy + 1, 50, f'Mean: {mean_life_expectancy:.2f}', color='red')
plt.xlabel('Life Expectancy')
plt.ylabel('Frequency')
plt.title('Life Expectancy Distribution')
plt.show()

# Plot a scatter plot between life expectancy and GDP per capita using Matplotlib
plt.scatter(gapminder['gdpPercap'], gapminder['lifeExp'], alpha=0.5)
plt.axhline(mean_life_expectancy, color='red', linestyle='dashed', linewidth=1)
plt.axvline(median_gdp_per_capita, color='green', linestyle='dashed', linewidth=1)
plt.text(median_gdp_per_capita + 500, mean_life_expectancy + 1, f'Median: {median_gdp_per_capita:.2f}', color='green')
plt.xlabel('GDP per Capita')
plt.ylabel('Life Expectancy')
plt.title('GDP per Capita vs Life Expectancy')
plt.show()
```

<!-- In the code above, we start by loading the Gapminder data using the `pd.read_csv()` function from Pandas. Next, we calculate the mean life expectancy and the median GDP per capita using NumPy functions `np.mean()` and `np.median()`, respectively. To visualize the distribution of life expectancy, we plot a histogram using the `plt.hist()` function from Matplotlib. -->

<!-- ## Exercises -->
